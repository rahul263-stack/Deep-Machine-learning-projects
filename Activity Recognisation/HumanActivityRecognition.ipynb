{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "These days Smartphones have become an integral part of our life. We cannot assume our life without a mobile phone. Since, the advent of Smartphones, a revolution has been created in the mobile communication industry. Smartphones are not just restricted for calling these days. Infact, they are more often used for entertainment purpose.<br><br>\n",
    "Smartphone manufacturing companies load Smartphones with various sensors to enhance the user experinece. Two of the such sensors are <b>Accelerometer</b> and <b>Gyroscope</b>. <b>Accelerometer</b> measures acceleration while <b>Gyroscope</b> measures angular velocity.<br><br>\n",
    "Here, we will try to use the data provided by accelerometer and gyroscope of Smartphone to classify the activity which a Smartphone user is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why this is Useful?\n",
    "These days, in addition to Smartphones, we are also using Smart-Watches like Fitbit or Apple-Watch, which help us to track our health. They monitor our each activity throughout the day check how many calories we have burnt. How many hours have we slept. However, in addition to Accelerometer and Gyroscope, they also use Heart-Rate data to monitor our activity. Since, we only have Smartphone data so just by using  Accelerometer and Gyroscope data we will monitor the activity of a person. This software can then be converted into an App which can be downloaded in Smartphone. Hence, a person who has Smartphone can monitor his/her health using this App\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about Data\n",
    "### How Data is recorded\n",
    " The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities <b>(WALKING, WALKING-UPSTAIRS, WALKING-DOWNSTAIRS, SITTING-DOWN, STANDING-UP, LAYING-DOWN)</b> wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n",
    "5.2. Features\n",
    "1. These sensor signals are pre-processed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. i.e., each window has 128 readings. A 128 size vector is created from each window.\n",
    "2. From Each window or to be more precise, from each 128 readings domain experts from signal processing have engineered feature vector of size 561 by calculating variables from the time and frequency domain. In our dataset, each data-point represents a window with different readings.\n",
    "3. 561 features are stored in the file \"Features.docx\". Check it out. \n",
    "4. Check out 561 features here.(In your blog give here the link of the docx file of features which you upload on github).\n",
    "5. The acceleration signal was separated into Body and     Gravity acceleration signals(tBodyAcc-XYZ and tGravityAcc-XYZ) using some low pass filter with corner frequency of 0.3Hz.\n",
    "6. After that, the body linear acceleration and angular velocity were derived in time to obtain jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ).\n",
    "7. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag and tBodyGyroJerkMag.\n",
    "8. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labelled with prefix 'f' just like original signals with prefix 't'. These signals are labelled as fBodyAcc-XYZ, fBodyGyroMag etc.<br>\n",
    "\n",
    "These are the signals that we got so far.\n",
    "\n",
    "* tBodyAcc-XYZ\n",
    "* tGravityAcc-XYZ\n",
    "* tBodyAccJerk-XYZ\n",
    "* tBodyGyro-XYZ\n",
    "* tBodyGyroJerk-XYZ\n",
    "* tBodyAccMag\n",
    "* tGravityAccMag\n",
    "* tBodyAccJerkMag\n",
    "* tBodyGyroMag\n",
    "* tBodyGyroJerkMag\n",
    "* fBodyAcc-XYZ\n",
    "* fBodyAccJerk-XYZ\n",
    "* fBodyGyro-XYZ\n",
    "* fBodyAccMag\n",
    "* fBodyAccJerkMag\n",
    "* fBodyGyroMag\n",
    "* fBodyGyroJerkMag<br><br>\n",
    "\n",
    "9 We can estimate some set of variables from the above signals. i.e., We will estimate the following properties on each and every signal that we recorded so far.\n",
    "* mean(): Mean value\n",
    "* std(): Standard deviation\n",
    "* mad(): Median absolute deviation\n",
    "* max(): Largest value in array\n",
    "* min(): Smallest value in array\n",
    "* sma(): Signal magnitude area\n",
    "* energy(): Energy measure. Sum of the squares divided by the number of values.\n",
    "* iqr(): Inter-quartile range\n",
    "* entropy(): Signal entropy\n",
    "* arCoeff(): Auto-regression coefficients with Burg order equal to 4\n",
    "* correlation(): correlation coefficient between two signals\n",
    "* maxInds(): index of the frequency component with largest magnitude\n",
    "* meanFreq(): Weighted average of the frequency components to obtain a mean frequency\n",
    "* skewness(): skewness of the frequency domain signal\n",
    "* kurtosis(): kurtosis of the frequency domain signal\n",
    "* bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
    "* angle(): Angle between to vectors.<br><br>\n",
    "\n",
    "10 We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable.\n",
    "* gravityMean\n",
    "* tBodyAccMean\n",
    "* tBodyAccJerkMean\n",
    "* tBodyGyroMean\n",
    "* tBodyGyroJerkMean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "Data is downloaded from following source:<br>\n",
    "https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview of Dataset\n",
    "Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.<br><br>\n",
    "<b>These activites are encoded as follows:<br>\n",
    "WALKING-- 1<br>\n",
    "WALKING_UPSTAIRS-- 2<br>\n",
    "WALKING_DOWNSTAIRS-- 3<br>\n",
    "SITTING-- 4<br>\n",
    "STANDING-- 5<br>\n",
    "LYING-- 6<br>\n",
    "</b>\n",
    "* Readings are divided into a window of 2.56 seconds with 50% overlapping.\n",
    "* Accelerometer readings are divided into gravity acceleration and body acceleration readings, which has x, y and z components each.\n",
    "* Gyroscope readings are the measure of angular velocities which has x, y and z components.\n",
    "* Jerk signals are calculated for Body-Acceleration readings.\n",
    "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
    "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, energy-bands, entropy etc., are calculated for each window.\n",
    "* Extra features are calculated by taking the average of signals in a single window sample. These are used on the angle() variable.\n",
    "* Finally, we got feature vector of 561 features and these features are given in the dataset.\n",
    "* Each window of readings is a data-point of 561 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y-Encoded Labels\n",
    "WALKING-- 1<br>\n",
    "WALKING_UPSTAIRS-- 2<br>\n",
    "WALKING_DOWNSTAIRS-- 3<br>\n",
    "SITTING-- 4<br>\n",
    "STANDING-- 5<br>\n",
    "LYING-- 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "Work-flow is as follows:<br>\n",
    "1. Domain experts from the field of Signal Processing collects the data from Accelerometer and Gyroscope of Smartphone.\n",
    "2. They break up the data in the time window of 2.56 seconds with 50% overlapping i.e., 128 reading\n",
    "3. They engineered 561 features from each time window of 2.56 seconds.<br>\n",
    "\n",
    "<b>By using either human engineered 561 feature data or raw features of 128 reading, our goal is to predict one of the six activities that a Smartphone user is performing at that 2.56 Seconds time window</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "<br> By using either human engineered 561 feature data or raw features of 128 reading, our goal is to predict one of the six activities that a Smartphone user is performing at that 2.56 Seconds time window.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Constraints\n",
    "1. No Low latency requirement.\n",
    "2. Errors are not much costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Problem Formulation\n",
    "All of the Accelerometer and Gyroscope are tri-axial, means that they measure acceleration and angular-velocity respectively in all the three axis namely X-axis, Y-axis and Z-axis. So, we have in total six time-series data. Given this six time-series data, we want to predict six activities namely <b>Walking</b> or <b>Walking-Upstairs</b> or <b>Walking-Downstairs</b> or <b>Lying-Down</b> or <b>Standing-Up</b> or <b>Sitting-Down</b>.<br><br>\n",
    "<b>At the outset, this is a multi-class classification problem.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric\n",
    "1. We will use Accuracy as one of the metric.\n",
    "2. We will also use Confusion-Matrix to check that in which two activities our model is confused and predicting incorrect activity. For example, between Standing-Up and Sitting-Down. Between Walking-Upstairs and Walking-Downstairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "All the data is present in 'UCI_HAR_dataset/' folder in present working directory.<br>\n",
    "Feature names are present in 'UCI_HAR_dataset/features.txt'<br><br>\n",
    "<b>Train Data</b><br>\n",
    "'UCI_HAR_dataset/train/X_train.txt'<br>\n",
    "'UCI_HAR_dataset/train/subject_train.txt'<br>\n",
    "'UCI_HAR_dataset/train/y_train.txt'<br>\n",
    "<b>Test Data</b><br>\n",
    "'UCI_HAR_dataset/test/X_test.txt'<br>\n",
    "'UCI_HAR_dataset/test/subject_test.txt'<br>\n",
    "'UCI_HAR_dataset/test/y_test.txt'<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Points Distribution\n",
    "* 30 test-subjects data is randomly split to 70%(21) train and 30%(7) test data.\n",
    "* Each data-point corresponds one of the 6 Activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of Action\n",
    "* We will apply classical Machine Learning models on these 561 sized domain expert engineered features. \n",
    "* As we know that LSTM works well on time-series data, so we have decided that we will apply LSTM of Recurrent Neural Networks on 128 sized raw readings that we obtained from accelerometer and gyroscope signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperas\n",
      "  Downloading hyperas-0.4.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: jupyter in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperas) (5.1.3)\n",
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.5-py2.py3-none-any.whl (965 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperas) (0.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperas) (6.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperopt->hyperas) (1.20.3)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperopt->hyperas) (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperopt->hyperas) (4.61.2)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.6.2-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: six in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from hyperopt->hyperas) (1.16.0)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter->hyperas) (5.1.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter->hyperas) (6.2.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter->hyperas) (6.4.3)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter->hyperas) (7.6.3)\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (7.26.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (0.1.2)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (6.1.12)\n",
      "Requirement already satisfied: traitlets<6.0,>=4.1.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (5.0.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipykernel->jupyter->hyperas) (1.4.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.4.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (3.0.17)\n",
      "Requirement already satisfied: pygments in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter-client<8.0->ipykernel->jupyter->hyperas) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter-client<8.0->ipykernel->jupyter->hyperas) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter-client<8.0->ipykernel->jupyter->hyperas) (4.7.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel->jupyter->hyperas) (228)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel->jupyter->hyperas) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from traitlets<6.0,>=4.1.0->ipykernel->jupyter->hyperas) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipywidgets->jupyter->hyperas) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbformat->hyperas) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->hyperas) (0.18.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from notebook->jupyter->hyperas) (0.11.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from notebook->jupyter->hyperas) (20.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from notebook->jupyter->hyperas) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from notebook->jupyter->hyperas) (3.0.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->hyperas) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from argon2-cffi->notebook->jupyter->hyperas) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->hyperas) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from jinja2->notebook->jupyter->hyperas) (2.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (4.0.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (0.5.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (1.4.3)\n",
      "Requirement already satisfied: testpath in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbconvert->hyperas) (0.5.0)\n",
      "Requirement already satisfied: async-generator in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas) (1.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from bleach->nbconvert->hyperas) (21.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
      "Requirement already satisfied: qtpy in c:\\users\\rahul\\miniconda3\\envs\\ml\\lib\\site-packages (from qtconsole->jupyter->hyperas) (1.10.0)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=7991a8db578c91ae53d5474c91ced2ed1f89684225e69943fe8fbb0d6e754f13\n",
      "  Stored in directory: c:\\users\\rahul\\appdata\\local\\pip\\cache\\wheels\\2f\\a0\\d3\\4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "Successfully built future\n",
      "Installing collected packages: networkx, future, cloudpickle, keras, hyperopt, hyperas\n",
      "Successfully installed cloudpickle-1.6.0 future-0.18.2 hyperas-0.4.1 hyperopt-0.2.5 keras-2.6.0 networkx-2.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\Miniconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2.__internal__' has no attribute 'register_clear_session_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27208/2694547495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;31m# Inject the clear_session function to keras_deps to remove the dependency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;31m# from TFLite to Keras.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_clear_session_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2.__internal__' has no attribute 'register_clear_session_function'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "with open(\"Features.txt\") as f:\n",
    "    for line in f:\n",
    "        features.append(line.split()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"X_train.txt\")\n",
    "\n",
    "train_df[\"subject_id\"] = pd.read_csv(\"subject_train.txt\", header = None, squeeze = True) #squeeze = True will \n",
    "#return data in pandas series format\n",
    "\n",
    "train_df[\"activity\"] = pd.read_csv(\"y_train.txt\", header = None, squeeze = True)\n",
    "\n",
    "activity = pd.read_csv(\"y_train.txt\", header = None, squeeze = True)\n",
    "\n",
    "#mapping activity to activity name\n",
    "label_name = activity.map({1: \"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LYING\"})\n",
    "\n",
    "train_df[\"activity_name\"] = label_name\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Train data = {}\".format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../Data/test/X_test.txt\", delim_whitespace = True, names = features)\n",
    "\n",
    "test_df[\"subject_id\"] = pd.read_csv(\"../Data/test/subject_test.txt\", header = None, squeeze = True) #squeeze = True will \n",
    "#return data in pandas series format\n",
    "\n",
    "test_df[\"activity\"] = pd.read_csv(\"../Data/test/y_test.txt\", header = None, squeeze = True)\n",
    "\n",
    "activity = pd.read_csv(\"../Data/test/y_test.txt\", header = None, squeeze = True)\n",
    "\n",
    "#mapping activity to activity name\n",
    "label_name = activity.map({1: \"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LYING\"})\n",
    "\n",
    "test_df[\"activity_name\"] = label_name\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Test data = {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nan values\n",
    "print(\"Number of NaN values in train data is \"+str(train_df.isnull().sum().sum()))\n",
    "print(\"Number of NaN values in test data is \"+str(test_df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate values\n",
    "print(\"Number of duplicate values in train data is \"+str(sum(train_df.duplicated())))\n",
    "print(\"Number of duplicate values in test data is \"+str(sum(test_df.duplicated())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking for imbalance in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_title(\"Activity by each test subject\", fontsize = 15)\n",
    "plt.tick_params(labelsize = 15)\n",
    "sns.countplot(x = \"subject_id\", hue = \"activity_name\", data = train_df)\n",
    "plt.xlabel(\"Subject ID\", fontsize = 15)\n",
    "plt.ylabel(\"Count\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_title(\"Count of each activity\", fontsize = 15)\n",
    "plt.tick_params(labelsize = 10)\n",
    "sns.countplot(x = \"activity_name\", data = train_df)\n",
    "for i in ax.patches:\n",
    "    ax.text(x = i.get_x() + 0.2, y = i.get_height()+10, s = str(i.get_height()), fontsize = 20, color = \"grey\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Count\", fontsize = 15)\n",
    "plt.tick_params(labelsize = 13)\n",
    "plt.xticks(rotation = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation</b><br>\n",
    "From the above two plots, we can infer that our classes are almost balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Changing Feature Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = columns.str.replace(\"[()]\", '') \n",
    "columns = columns.str.replace(\"-\", '')\n",
    "columns = columns.str.replace(\",\", '')\n",
    "#here, columns is of type pandas index. By writing \"columns.str\" we have changed its type to \n",
    "#pandas string. Pandas string has method called replace which we have used here.\n",
    "\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving Dataframe for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../Data/train/train_df.csv\", index = False)\n",
    "test_df.to_csv(\"../Data/test/test_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/train/train_df.csv\")\n",
    "test_df = pd.read_csv(\"../Data/test/test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature information from domain knowledge</b>\n",
    "1. <b>Static:</b> We have three types static features where test subject is in rest:\n",
    "    * Sitting\n",
    "    * Standing\n",
    "    * Lying\n",
    "2. <b>Dynamic:</b> We have three types of dynamic features where test subject is in motion:\n",
    "    * Walking\n",
    "    * Walking_Downstairs\n",
    "    * Walking_Upstairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of Body Accelerator Mean Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(data = train_df, hue = \"activity_name\", size = 8)\n",
    "facetgrid.map(sns.distplot, \"tBodyAccMagmean\", hist = False).add_legend()\n",
    "plt.annotate('Static Activities(Sitting, Standing, Lying)', xy=(-0.97, 23), xytext=(-0.7, 27),\n",
    "            arrowprops=dict(facecolor='orange', width = 7, headlength = 15), size = 15, color = \"#232b2b\")\n",
    "plt.annotate('Dynamic Activities(Walking, Walking_Upstairs, Walking_Downstairs)', xy=(0.1, 3), xytext=(0.4, 6),\n",
    "            arrowprops=dict(facecolor='orange', width = 7, headlength = 13), size = 15, color = \"#232b2b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot \"tBodyAccMagmean\" for both static and dynamic activites separately to analysis them in more detail\n",
    "df_standing = train_df[train_df[\"activity_name\"] == \"STANDING\"]\n",
    "df_sitting = train_df[train_df[\"activity_name\"] == \"SITTING\"]\n",
    "df_lying = train_df[train_df[\"activity_name\"] == \"LYING\"]\n",
    "df_walking = train_df[train_df[\"activity_name\"] == \"WALKING\"]\n",
    "df_walking_upstairs = train_df[train_df[\"activity_name\"] == \"WALKING_UPSTAIRS\"]\n",
    "df_walking_downstairs = train_df[train_df[\"activity_name\"] == \"WALKING_DOWNSTAIRS\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (14, 7))\n",
    "\n",
    "axes[0].set_title(\"Static Activities for tBodyAccMagmean--Zoomed In\")\n",
    "sns.distplot(df_standing[\"tBodyAccMagmean\"], hist = False, label = \"STANDING\", ax = axes[0])\n",
    "sns.distplot(df_sitting[\"tBodyAccMagmean\"], hist = False, label = \"SITTING\", ax = axes[0])\n",
    "sns.distplot(df_lying[\"tBodyAccMagmean\"], hist = False, label = \"Lying\", ax = axes[0])\n",
    "axes[0].legend(fontsize = 15)\n",
    "axes[0].annotate('Static Activities', xy=(-0.90, 15), xytext=(-0.7, 27),\n",
    "            arrowprops=dict(facecolor='orange', width = 7, headlength = 15), size = 15, color = \"#232b2b\")\n",
    "\n",
    "axes[1].set_title(\"Dynamic Activities for tBodyAccMagmean--Zoomed In\")\n",
    "sns.distplot(df_walking[\"tBodyAccMagmean\"], hist = False, label = \"WALKING\", ax = axes[1])\n",
    "sns.distplot(df_walking_upstairs[\"tBodyAccMagmean\"], hist = False, label = \"WALKING_UPSTAIRS\", ax = axes[1])\n",
    "sns.distplot(df_walking_downstairs[\"tBodyAccMagmean\"], hist = False, label = \"WALKING_DOWNSTAIRS\", ax = axes[1])\n",
    "axes[1].legend(fontsize = 15)\n",
    "axes[1].annotate('Dynamic Activities', xy=(0.37, 1.5), xytext=(0.60, 2.2),\n",
    "            arrowprops=dict(facecolor='orange', width = 7, headlength = 13), size = 15, color = \"#232b2b\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation</b><br>\n",
    "From above two plots we can clearly observe that how well \"tBodyAccMagmean\"--which is the magnitude of the mean of body acceleration in time-domain meaured by accelerometer--is able to separate static activity from dynamic activity. This shows that features are very carefully engineered by domian experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "sns.boxplot(x = \"activity_name\", y = \"tBodyAccMagmean\", showfliers = False, data = train_df)\n",
    "plt.axhline(y = -0.65, linestyle = \"--\")\n",
    "plt.axhline(y = 0, linestyle = \"--\")\n",
    "plt.title(\"Box plot of tBodyAccMagmean\", fontsize = 15)\n",
    "plt.ylabel(\"Accelerator Body Mean\", fontsize = 15)\n",
    "plt.xlabel(\"Activity Name\", fontsize = 15)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(labelsize = 15)\n",
    "plt.xticks(rotation = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations:</b><br>\n",
    "* If tAccMean is < -0.8 then the Activities are either Standing or Sitting or Laying.\n",
    "* If tAccMean is > -0.6 then the Activities are either Walking or WalkingDownstairs or WalkingUpstairs.\n",
    "* If tAccMean > 0.0 then the Activity is WalkingDownstairs.\n",
    "* We can classify 75% the Acitivity labels with some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator Gravity Mean on X-axis can be quite important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "sns.boxplot(x = \"activity_name\", y = \"angleXgravityMean\", showfliers = True, data = train_df)\n",
    "plt.axhline(y = 0, linestyle = \"--\")\n",
    "plt.title(\"Box plot of tBodyAccMagmean\", fontsize = 15)\n",
    "plt.ylabel(\"Accelerator Gravity Mean on X-axis\", fontsize = 15)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(labelsize = 15)\n",
    "plt.xticks(rotation = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation</b><br>\n",
    "* If Acc Gravity Mean > 0, we can infer that the activity will most likely be <b>Lying</b>.\n",
    "* If Acc Gravity Mean < 0, we can infer that the activity can be anything but <b>Lying</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Applying T-SNE on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_tsne(perplexity, train_df):\n",
    "    data = train_df.drop([\"subject_id\", \"activity\", \"activity_name\"], axis = 1)\n",
    "    data_label = train_df[\"activity_name\"]\n",
    "    applying_tsne = TSNE(n_components = 2, perplexity = perplexity, n_iter = 1000, verbose = 2)\n",
    "    reduced_dim = applying_tsne.fit_transform(data)\n",
    "    d = {'Dimension_1': applying_tsne.embedding_[:,0], 'Dimension_2': applying_tsne.embedding_[:,1], \"activities\":data_label}\n",
    "    df = pd.DataFrame(data = d)\n",
    "    print(\"Done...\")\n",
    "    print(\"Plotting TSNE Visualization...\")\n",
    "    sns.set_style('whitegrid') \n",
    "    sns.lmplot(\"Dimension_1\", \"Dimension_2\", df, hue = 'activities', markers=['|','o','_', \">\", \"<\", \"^\"], fit_reg = False, size = 10, scatter_kws={'s':100})\n",
    "    plt.title(\"TSNE Plot for Perplexity \"+str(perplexity))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perplexities = [5, 10, 20, 40, 100]\n",
    "for perplexity in perplexities:\n",
    "    plt_tsne(perplexity, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation</b><br>\n",
    "From above TSNE plots, we can observe that except <b>STANDING</b> and <b>SITTING</b>, all other activities are separated fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop([\"subject_id\", \"activity\", \"activity_name\"], axis = 1)\n",
    "y_train = train_df[\"activity\"]\n",
    "\n",
    "x_test = test_df.drop([\"subject_id\", \"activity\", \"activity_name\"], axis = 1)\n",
    "y_test = test_df[\"activity\"]\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(columns = [\"Model\", \"Accuracy(%)\"])\n",
    "def keeping_record(model_name, accuracy):\n",
    "    global table\n",
    "    table = table.append(pd.DataFrame([[model_name, accuracy]], columns = [\"Model\", \"Accuracy(%)\"]))\n",
    "    table.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusionMatrix(Y_TestLabels, PredictedLabels):\n",
    "    confusionMatx = confusion_matrix(Y_TestLabels, PredictedLabels)\n",
    "    \n",
    "    precision = confusionMatx/confusionMatx.sum(axis = 0)\n",
    "    \n",
    "    recall = (confusionMatx.T/confusionMatx.sum(axis = 1)).T\n",
    "    \n",
    "    sns.set(font_scale=1.5)\n",
    "    \n",
    "    # confusionMatx = [[1, 2],\n",
    "    #                  [3, 4]]\n",
    "    # confusionMatx.T = [[1, 3],\n",
    "    #                   [2, 4]]\n",
    "    # confusionMatx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # confusionMatx.sum(axix =1) = [[3, 7]]\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)) = [[1/3, 3/7]\n",
    "    #                                                  [2/3, 4/7]]\n",
    "\n",
    "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)).T = [[1/3, 2/3]\n",
    "    #                                                    [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    labels = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LYING\"]\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(confusionMatx, cmap = \"Blues\", annot = True, fmt = \".1f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Confusion Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(precision, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Precision Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\"*125)\n",
    "    \n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.heatmap(recall, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Recall Matrix\", fontsize = 30)\n",
    "    plt.xlabel('Predicted Class', fontsize = 20)\n",
    "    plt.ylabel('Original Class', fontsize = 20)\n",
    "    plt.tick_params(labelsize = 15)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(cross_val, x_train, y_train, x_test, y_test, model_name):\n",
    "    start = datetime.now()\n",
    "    cross_val.fit(x_train, y_train)\n",
    "    predicted_points = cross_val.predict(x_test)\n",
    "    \n",
    "    print(\"Total time taken for tuning hyperparameter and making prediction by the model is (HH:MM:SS): {}\\n\".format(datetime.now() - start))\n",
    "    accuracy = np.round(accuracy_score(y_test, predicted_points)*100, 2)\n",
    "    \n",
    "    print('---------------------')\n",
    "    print('|      Accuracy      |')\n",
    "    print('---------------------')\n",
    "    print(str(accuracy)+\"%\\n\")\n",
    "    \n",
    "    print('---------------------------')\n",
    "    print('|      Best Estimator      |')\n",
    "    print('---------------------------')\n",
    "    print(\"{}\\n\".format(cross_val.best_estimator_))\n",
    "    \n",
    "    print('----------------------------------')\n",
    "    print('|      Best Hyper-Parameters      |')\n",
    "    print('----------------------------------')\n",
    "    print(cross_val.best_params_)\n",
    "    \n",
    "    keeping_record(model_name, accuracy)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    print_confusionMatrix(y_test, predicted_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10**1, 10**2, 10**3], \"penalty\": [\"l1\", \"l2\"]}\n",
    "clf = LogisticRegression(multi_class = \"ovr\")\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10**1, 10**2, 10**3]}\n",
    "clf = LinearSVC()\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"Linear SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"C\": [0.001, 0.01, 0.1, 1, 10**1, 10**2, 10**3]}\n",
    "clf = SVC()\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"RBF SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": [2, 3, 4, 5, 6, 7, 8]}\n",
    "clf = DecisionTreeClassifier()\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"Decision Trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\": [50, 100, 200, 400, 800]}\n",
    "clf = RandomForestClassifier()\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\": [50, 100], \"max_depth\":[1, 3]}\n",
    "clf = GradientBoostingClassifier()\n",
    "cross_val = GridSearchCV(clf, parameters, cv=3)\n",
    "apply_model(cross_val, x_train, y_train, x_test, y_test, \"Gradient Boosted DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = table.plot(x = \"Model\", y = \"Accuracy(%)\", kind = \"bar\", figsize = (14, 7), legend = False)\n",
    "plt.title(\"Models Accuracy Score\", fontsize = 20)\n",
    "plt.xlabel(\"\")\n",
    "plt.margins(x = 0, y = 0.08)\n",
    "plt.ylabel(\"Accuracy Score(%)\", fontsize = 20)\n",
    "plt.grid(visible = True)\n",
    "for i in ax.patches:\n",
    "    ax.text(x = i.get_x()+0.05, y = i.get_height()+1, s = str(i.get_height())+\"%\", fontsize = 16, color = \"#232b2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comments</b><br>\n",
    "* Models: Logistic Regression, rbf SVM and Linear SVM give accuracy above 96%.\n",
    "* In real world, having domain knowledge is one of the most important aspects of machine learning Modelling. Here, we got pretty good accuracy of above 96%. This is very much due to the fact that features are very well engineered by domain experts in signal processing.<br>\n",
    "* In a nutshell, feature engineering is one of the most important aspect of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Applying Deep Learning Model: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Here in LSTM, we will use 128 sized raw readings that we obtained from accelerometer and gyroscope signals.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals_list = [\"body_acc_x_\", \"body_acc_y_\", \"body_acc_z_\", \"body_gyro_x_\", \"body_gyro_y_\", \"body_gyro_z_\", \n",
    "                   \"total_acc_x_\", \"total_acc_y_\", \"total_acc_z_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace = True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_signal_matrix(trainOrTest):\n",
    "    complete_data = []\n",
    "    for signal in all_signals_list:\n",
    "        complete_data.append(reading_data(\"../Data/\"+ trainOrTest +\"/Inertial Signals/\"+ signal + trainOrTest +\".txt\").as_matrix())\n",
    "    return np.transpose(complete_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(subset):\n",
    "    filename = \"../Data/\"+subset+\"/y_\"+subset+\".txt\"\n",
    "    y = reading_data(filename)\n",
    "    return pd.get_dummies(y[0]).as_matrix()\n",
    "# here, get_dummies takes pandas series as input and returns its one-hot encoded vector of each element in a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_data():\n",
    "    x_train = total_signal_matrix(\"train\")\n",
    "    y_train = load_labels(\"train\")\n",
    "    x_test = total_signal_matrix(\"test\")\n",
    "    y_test = load_labels(\"test\")\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_full_data()\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data for loading it later in hyperas for hyper-parameter tuning\n",
    "np.save(\"../Data/train\", x_train)\n",
    "np.save(\"../Data/train_label\", y_train)\n",
    "np.save(\"../Data/test\", x_test)\n",
    "np.save(\"../Data/test_label\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    x_train = np.load(\"../Data/train.npy\")\n",
    "    y_train = np.load(\"../Data/train_label.npy\")\n",
    "    x_test = np.load(\"../Data/test.npy\")\n",
    "    y_test = np.load(\"../Data/test_label.npy\")\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will return number of classes\n",
    "def count_unique_classes(y_train):\n",
    "    return len(set([tuple(a) for a in y_train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Hyper-Parameter Tuning with Hyperas and Applying LSTM with best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer documentation of hyperas here: https://github.com/maxpumperla/hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    epochs = 8\n",
    "    batch_size = 32\n",
    "    timesteps = x_train.shape[1]\n",
    "    input_dim = len(x_train[0][0])\n",
    "    n_classes = 6\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(64, return_sequences = True, input_shape = (timesteps, input_dim)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(LSTM({{choice([32, 16])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
    "    \n",
    "    result = model.fit(x_train, y_train, batch_size = batch_size, epochs=epochs, verbose=2, validation_split=0.01)\n",
    "    \n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    \n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model, data=data, algo=tpe.suggest, max_evals=4, trials=Trials(), notebook_name = \"HumanActivityRecognition\")\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "\n",
    "score = best_model.evaluate(x_test, y_test)\n",
    "\n",
    "print('---------------------')\n",
    "print('|      Accuracy      |')\n",
    "print('---------------------')\n",
    "acc = np.round((score[1]*100), 2)\n",
    "print(str(acc)+\"%\\n\")\n",
    "    \n",
    "print('----------------------------------')\n",
    "print('|      Best Hyper-Parameters      |')\n",
    "print('----------------------------------')\n",
    "print(best_run)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "true_labels = [np.argmax(i)+1 for i in y_test]\n",
    "predicted_probs = best_model.predict(x_test)\n",
    "predicted_labels = [np.argmax(i)+1 for i in predicted_probs]\n",
    "print_confusionMatrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Final Comments</b>\n",
    "* By Simple two layered LSTM, we got a good accuracy of 87.72%. In short, DeeP Learning help us to built models even when we don't have domain expert engineered features. \n",
    "* LSTM model can be further improved by running it for more epochs and more evaluations while tuning hyper-parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
